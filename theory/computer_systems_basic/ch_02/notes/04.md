[Back to index](../../main.md)

# 2.4 Floating Point
#### Target) IEEE Floating Point Format
* Virtually all computers support this.


## 2.4.1 Fractional Binary Numbers
#### Terminologies)
* integral values : 정수값
* fractional values : 소수값
* binary point
  * Symbol '.'
  * Bits on the left are weighted by positive powers of 2
  * Bits on the right are weighted by negative powers of 2

<p align="left">
  <img src="https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/images/02_04_01_binary_def.png" width="20%">
</p>


#### Prop.) Shifting the binary point
* leftward : dividing by 2
* rightward : multiplying by 2


#### Notation) 1 - ε

<p align="left">
  <img src="https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/images/02_04_01_epsilon.png" width="80%">
</p>

  
#### Prop.) Fractional binary notation cannot represent infinite-length encodings
* ex : 1/3, 5/7

  
#### Prop.) Fractional binary notation can only approximate finite-length values.

<p align="left">
  <img src="https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/images/02_04_01_approximation.png" width="40%">
</p>

[:orange_book: Practice Problem 2.45](https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/problems/practice_problems.md#-practice-problem-245)

[:orange_book: Practice Problem 2.46](https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/problems/practice_problems.md#-practice-problem-246)

---


## 2.4.2 IEEE Floating-Point Representation
#### Def.) IEEE floating-point standard

<p align="left">
  <img src="https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/images/02_04_02_ieee_def.png" width="80%">
</p>

* Terms.)
  * 2^E : the exponent part
  * M : fractional binary part


#### Concept) Two Common Formats
1. Single Precision (32-bit)
   * Usage
     * float data type in C language
   * Configuration
     * k =  8 bits
     * n = 23 bits
2. Double Precision (64-bit)
   * Usage
     * double data type in in C language
   * Configuration
     * k = 11 bits
     * n = 52 bits

<p align="left">
  <img src="https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/images/02_04_02_single_double_precision.png" width="80%">
</p>


### Concept) Three Cases of bit representation
* Criteria
  * Value of exp.
* Types
  1. [Normalized Values](https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/notes/04.md#1-normalized-values)
  2. [Denormalized Values](https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/notes/04.md#2-denormalized-values)
  3. [Special Values](https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/notes/04.md#3-special-values)

#### 1. Normalized Values
* Def.)
  * The bit pattern of the exp(2^E) is neither all zeroes nor all ones.
    * Not 0
    * Not 255 for single precision (8/32 bit)
    * Not 2047 for double precision (11/64 bit)
* Prop 1) The exponent part (2^E) represents a signed integer in biased form.
  * E = e - Bias
    * Where,
      * e : unsigined number having bit representation e_(k-1), e_(k-2), ... , e_1, e_0
      * Bias : 2^(k-1) -1
        * single precision : 127
        * double precision : 1023
    * Thus, the range of the value of E is...
      * single precision : -126 <= E <= 127
      * double precision : -1022 <= E <= 1023

<p align="left">
  <img src="https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/images/02_04_02_exponent_derivation.png" width="60%">
</p>

* Prop 2) The fractional part represents the fractional value f, where 0 <= f < 1.
  * f : f_(n-1), f_(n-2), ... , f_1, f_0
    * the __binary point__ at the left of the most significant bit
      * i.e.) the most significant bit : 2^(-1)
    * M = 1 + f
      * AKA significand
      * AKA implied leading 1 representation
        * Additional bit acquired by this trick.



#### 2. Denormalized Values
* Def.)
  * The bit pattern of the exp(2^E) is all zeroes.
* Prop.)
  * The exponent part : E = 1 - Bias
    * Here, Bias = 2^(k-1)
    * Distinguished from the _2^(k-1) - 1_ of the Normalized case.
    * More precisely, E = 1 - (2^(k-1) -1)
      * [Why Doing This?](https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/notes/04.md#prop-why-using-1-bias-instead-of--bias-for-the-denormalized-exponent-e)
  * The fractional binary part : M = f
    * No implied leading 1.
* Why using it?
  1. It provide a way to represent numeric value 0.
    * Normalized Values cannot represent 0.
      * why? 
        * M >= 1 (the implied leading 1).
    * Denormalized Value can represent 0.
      * How?
        * If fractional part is 0.
          * f_(n-1) = f_(n-2) = ... = f_1 = f_0
            * Then, f = 0.
            * Thus, M = 0.
            * Hence, V = (-1)^s * M * 2^E = 0
      * Depending on the sign bit s...
        1. s = 0; +0.0
        2. s = 1; -0.0
  2. It can represent values that are very close to 0.0.
     * How? 
       * gradual underflow

#### 3. Special Values
* Def.)
  * The bit pattern of the exp(2^E) is all ones.
* Prop.)
  * It represents infinite value.
    * Depending on the sign bit s...
      1. s = 0; +∞
      2. s = 1; -∞
  * It can cause overflow...
  * NaN (Not a Number)
    * When the fraction field is nonzero.
    * It represent values that cannot be given as a real number.

<p align="left">
  <img src="https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/images/02_04_02_ieee_desc_image.png" width="80%">
</p>

---

## 2.4.3 Example Numbers

<p align="left">
  <img src="https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/images/02_04_03_6bit_graphic.png" width="80%">
</p>
<p align="left">
  <img src="https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/images/02_04_03_6bit_pf.png" width="80%">
</p>




#### Prop.) Why using __1-Bias__ instead of __-Bias__ for the denormalized exponent E.
* Answer : Smooth transition between the normalized and the denormalized values.
* pf.) Check the 6-bit example above.
  
<p align="left">
  <img src="https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/images/02_04_03_6bit_pf_smooth.png" width="80%">
</p>


#### Prop.) The bit representation of IEEE Floating Point supports sorting just like the decimal value.
* Check the 8-bit example below.
  * The decimal and the bit representation are sorted with the same sequence

<p align="left">
  <img src="https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/images/02_04_03_8bit_ex.png" width="80%">
</p>


[:orange_book: Practice Problem 2.47](https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/problems/practice_problems.md#-practice-problem-247)


#### Ex.) 8-bit IEEE Floating point representations and some important values

<p align="left">
  <img src="https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/images/02_04_03_8bit_ex_points.png" width="80%">
  <img src="https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/images/02_04_03_8bit_ex_points1.png" width="80%">
  <img src="https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/images/02_04_03_8bit_ex_points2.png" width="80%">
</p>


#### Ex.) Converting 12345.0 into IEEE Format

<p align="left">
  <img src="https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/images/02_04_03_8bit_12345.png" width="80%">
</p>


#### Prop.) The correlation between the bit-level representation of the integer value and the single-precision floating point value.

<p align="left">
  <img src="https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/images/02_04_03_correlation.png" width="60%">
</p>


[:orange_book: Practice Problem 2.48](https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/problems/practice_problems.md#-practice-problem-248)

[:orange_book: Practice Problem 2.49](https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/problems/practice_problems.md#-practice-problem-249)

---

## 2.4.4 Rounding
#### Prop.) The IEEE floating-point format defines four different rounding modes.
1. Round-to-even
   * the default mode
   * Rule
     * It attempts to find the closest match.
     * In case of five (AKA halfway!), it attempts to round the number either upward or downward such that the least significant digit of the result is even.
       * ex
         * 1.5 -> 2 (Consider that 1 is odd.)
         * 2.5 -> 2
       * Why doing this?
         * Rounding toward even numbers avoids statistical bias when calculating the average value.
           * why?)
             * Rounding up 5 upward may overvalue the average.
   * Application to the binary numbers.
     * How?
       * Just like the decimal case, if the target value is exactly the halfway round toward 0, which is even!
         * ex) Rounding values to the nearest quarter.
           * 10.00011 -> 10.00 (NOT halfway)
           * 10.00110 -> 10.01 (NOT halfway)
           * 10.11100 -> 11.0**0** (Upward. Even for the quarter!)
           * 10.10100 -> 10.1**0** (Downward. Even for the quarter!)
2. Round-to-zero
   * Round positive numbers downward.
   * Round negative numbers upward.
3. Round-down
4. Round-up

<p align="left">
  <img src="https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/images/02_04_04_four_modes.png" width="60%">
</p>

[:orange_book: Practice Problem 2.50](https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/problems/practice_problems.md#-practice-problem-250)

[:orange_book: Practice Problem 2.51](https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/problems/practice_problems.md#-practice-problem-251)

[:orange_book: Practice Problem 2.52](https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/problems/practice_problems.md#-practice-problem-252)


---

## 2.4.5 Floating-Point Operations
#### Tech.) Floating-Point Addition
* Def.)

<p align="left">
  <img src="https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/images/02_04_05_addition_def.png" width="20%">
</p>

* Prop.) Floating-Point addition is commutative.
  * i.e.) x + y = y + x

* Prop.) Floating-Point addition is NOT associative.
  * i.e.) x + (y + z) != (x + y) + z
  * ex)
    * (3.14 + 1e10) - 1e10 = 0.0   /   3.14 + (1e10 - 1e10) = 3.14
      * Due to the rounding!
    * Check the following code!
```C
#include <stdio.h>

void not_associative_test(float a, float b, float c, float d){
    float x, y, t;
    x = a + b + c;
    y = b + c + d;
    printf("Case 1) x = %.50f, y = %.50f\n", x, y);
    t = b + c;
    x = a + t;
    y = t + d;
    printf("Case 2) x = %.50f, y = %.50f\n", x, y);
}

int main() {
    not_associative_test(3.14, 1e10, -1e10, 2.0);
}
```

* Prop.) Most values have inverses under floating-point addition. (But NOT All!)
  * Recall that [every integer was in the Abelian Group](https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/notes/03.md#prop-abelian-group-structure-of-the-modular-addition).
  * Not for some of the floating-point values.
    * ex.)
      * +∞ − ∞ = NaN
      * NaN + x = NaN, for any x.

* Prop.) Floating-Point addition satisfies the monotonicity.
  * If a ≥ b then x + a ≥ x + b for any values of a, b, and x other than NaN.
    * Which was [not satisfied in the integer arithmetics](https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/problems/practice_problems.md#-practice-problem-244).



#### Tech.) Floating-Point Multiplication
* Def.)

<p align="left">
  <img src="https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/images/02_04_05_multiplication_def.png" width="20%">
</p>

* Prop.) Floating-Point multiplication is commutative.
  * i.e.) x * y = y * x

* Prop.) Floating-Point multiplication is NOT associative.
  * i.e.) x * (y * z) != (x * y) * z
  * Due to the possibility of the overflow or the loss of precision
  * ex)
    * (1e20 \* 1e20) \* 1e-20 = +∞ /  1e20 \* (1e20 \* 1e-20) = 1e20

* Prop.) Floating-Point addition satisfies the monotonicity.

<p align="left">
  <img src="https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/images/02_04_05_multiplication_monotonicity.png" width="30%">
</p>


* Prop.) 
  * Recall that the following was not satisfied in the integer arithmetics.

<p align="left">
  <img src="https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/images/02_04_05_multiplication_non_negative.png" width="30%">
</p>


---

## 2.4.6 Floating Point in C
* Props.)
  * All versions of C provide two different floating-point data types : float and double
    * float corresponds to single-precision floating point of IEEE floating point.
    * double corresponds to double-precision floating point of IEEE floating point.
  * Machines use the round-to-even mode.
  * There are no standard methods to change the rounding mode or to get special values such as −0, +∞, −∞, or NaN.
    * why?) The C standards do not require the machine to use IEEE floating point
      * In case of GCC, they are defined in <math.h>
  * C99 supports long double data type.
    * For many machines and compilers, this data type is equivalent to the double data type. 
    * For Intel-compatible machines, gcc implements this data type using an 80-bit “extended precision” format.
    * Although *long double* data type requires 10 bytes, GCC allocates 12 bytes.
      * Check [Practice Problem 3.35](https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_03/problems/practice_problems.md#-practice-problem-335)


[:orange_book: Practice Problem 2.53](https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/problems/practice_problems.md#-practice-problem-253)


#### Tech.) Casting values between int, float, and double
1. int -> float
   * No overflow, but may be rounded.
2. int or float -> double
   * The exact value will be preserved.
     * why?) double supports a wider range!
3. double -> float
   * The value can overflow to +∞ or −∞
     * why?) float's range is smaller compared to double.
   * May be rounded.
     * why?) float's precision is smaller compared to double.
4. float or double -> int
   * The value may overflow.
   * The value will be rounded to zero.
     * ex)
       * -1.9 -> -1
       * 2.1 -> 2


[:orange_book: Practice Problem 2.54](https://github.com/JoonHyeok-hozy-Kim/computer_systems_study/blob/main/contents/ch_02/problems/practice_problems.md#-practice-problem-254)




### KEYWORDS
* fractional binary numbers; binary point; IEEE floating-point standard; single precision format; double precision format; exponent part; fractional binary part; significand(정수부); normalized value; implied leading one(1); denormalized value; special value; gradual underflow; Not a Number(NaN); round-to-even; round-toward-zero; round-down; round-up; floating-point addition; floating-point multiplication; float data type; double data type; long double data type(C99); casting between float/double and int;


[Back to index](../../main.md)